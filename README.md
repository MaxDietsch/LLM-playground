# Transformer from Scratch with PyTorch

This project implements a Transformer model from scratch using PyTorch. It is designed as a learning resource to understand the inner workings of Transformers and large language models (LLMs).

## Features
- **From-Scratch Implementation**: Dive deep into the architecture and mechanics of Transformers, including self-attention, positional embeddings, and feedforward layers.
- **PyTorch-Based**: Leverage the flexibility and power of PyTorch for building and experimenting with models.
- **Educational Focus**: Ideal for those looking to gain a solid understanding of how Transformers and LLMs work.

## Project Goals
The primary goal of this project is to demystify Transformers and LLMs by providing a step-by-step implementation. It serves as a practical guide for:
- Understanding the key components of Transformer models.
- Gaining insights into the design and training of modern deep learning architectures.

## Dependencies
This project uses **Poetry** for dependency management and environment configuration. Poetry ensures that dependencies are cleanly handled and the project environment is reproducible.

## Setup Instructions
1. **Install Poetry**:
   If you donâ€™t already have Poetry installed, you can install it by following the [official instructions](https://python-poetry.org/docs/#installation).

2. **Clone the Repository**:
   ```bash
   git clone <repository-url>
   cd <repository-folder>
